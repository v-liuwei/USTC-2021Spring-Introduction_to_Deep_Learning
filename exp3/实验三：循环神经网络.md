# 实验三：循环神经网络

姓名：刘威

学号：PB18010469


## 实验目的

* 了解并熟悉循环神经网络的原理
* 了解随时间反向传播算法（BPTT）
* 学会使用循环神经网络完成文本分类任务


## 实验原理

![image-20210513172244821](%E5%AE%9E%E9%AA%8C%E4%B8%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210513172244821.png)

![image-20210513172303845](%E5%AE%9E%E9%AA%8C%E4%B8%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210513172303845.png)

![image-20210513172317856](%E5%AE%9E%E9%AA%8C%E4%B8%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210513172317856.png)

![image-20210513172733303](%E5%AE%9E%E9%AA%8C%E4%B8%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210513172733303.png)



![image-20210513172747548](%E5%AE%9E%E9%AA%8C%E4%B8%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210513172747548.png)

![image-20210513172852144](%E5%AE%9E%E9%AA%8C%E4%B8%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210513172852144.png)

![image-20210513172947008](%E5%AE%9E%E9%AA%8C%E4%B8%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210513172947008.png)



![image-20210513173012590](%E5%AE%9E%E9%AA%8C%E4%B8%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210513173012590.png)

![image-20210513173151864](%E5%AE%9E%E9%AA%8C%E4%B8%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210513173151864.png)



![image-20210513173355442](%E5%AE%9E%E9%AA%8C%E4%B8%89%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210513173355442.png)


## 实验内容

编写RNN的语言模型，并基于训练好的词向量，编写RNN模型用于文本分类

数据集：aclIMDB

预训练词向量：GloVe.6B

## 实验结果

实验使用`pytorch`进行


### 源码结构及说明

#### 数据预处理部分

使用`torchtext`库处理文本；使用`spaCy`库进行分词。

将`train/`目录下的数据集划分为`train/validation`, 划分比例为`0.8/0.2`.

#### 模型部分

由一个`Embedding`层和一个`RNN/LSTM`模块构成，后者可以调节层数和是否双向。

`Embedding`层使用`GloVe`预训练词向量进行初始化。

模型定义如下：

```python
class RNNClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim,
                 n_layers: int = 1, bidirectional: bool = False,
                 dropout: float = 0., model_base: str = 'RNN'):
        super(RNNClassifier, self).__init__()
        self.bidirectional = bidirectional
        self.model_base = model_base.lower()
        if self.model_base == 'lstm':
            model = nn.LSTM
        else:
            model = nn.RNN

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = model(embedding_dim,
                         hidden_dim,
                         num_layers=n_layers,
                         bidirectional=bidirectional,
                         dropout=dropout)
        if self.bidirectional:
            hidden_dim *= 2
        self.fc = nn.Linear(hidden_dim, 1)
        self.act = nn.Sigmoid()

    def forward(self, x, x_len):
        x = self.embedding(x)
        x = pack_padded_sequence(x, x_len)
        if self.model_base == 'lstm':
            _, (h_n, _) = self.rnn(x)
        else:
            _, h_n = self.rnn(x)  # h_n.shape = (num_layers * num_directions, batch, hidden_size)
        if self.bidirectional:
            hidden = torch.cat((h_n[-2], h_n[-1]), dim=1)  # get last layer
        else:
            hidden = h_n[-1]
        logits = self.fc(hidden)
        output = self.act(logits)
        return output
```

### 结果及分析

本实验的可选参数为

```python
VOCAB_SIZE = 400000
EMBEDDING_DIM = 100
HIDDEN_DIM = 64
N_LAYERS = 1  # RNN/LSTM 层数
BIDIRECTIONAL = False  # 是否双向
DROPOUT = 0.
BATCH_SIZE = 128
N_EPOCHS = 10
MODEL_BASE = 'RNN'  # 使用`Elman RNN` 还是 `LSTM`
```

此外，本实验固定随机种子：

```python
import torch
import random
import os
import numpy as np

def set_seed(seed=123):
    random.seed(seed)
    np.random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # torch.use_deterministic_algorithms(True)
    # torch.backends.cudnn.enabled = False
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:2"
    
set_seed(2077)
```

**词向量维度：100维**

在验证集上验证保存`val_loss`最低的模型用于测试，得到的测试集准确率(%)如下表:

|N_L-N_D|RNN| LSTM|
|:---:|:---:|:---:|
| 1-1 |77.22|85.57|
| 1-2 |77.87| 85.64 |
| 2-1 | 77.30 | 86.56 |
| 2-2 | 76.41 | 85.65 |
| 5-1 | 75.28 | 84.47 |
| 5-2 | 74.14 | 84.56 |

**注：**`N_L`代表`N_Layers`, 及循环神经网络的层数， `N_D`代表`N_Direction`, 当`bidirectional`设为`False`时为 `1`， 否则为 `2`.

结果表明，`LSTM`明显优于`RNN`；设置双向对于网络浅时略有提升，对于网络深时有副作用；简单地加深网络会使模型性能变差。

**词向量维度：300维**

将词向量维度增加到300维，比较模型表现。

| N_L-N_D |  RNN  | LSTM  |
| :-----: | :---: | :---: |
|   1-1   | 76.29 | 86.20 |
|   1-2   | 76.80 | 86.62 |

RNN性能变差，LSTM性能变好。


## 实验总结

本实验地主要难点在于：

+ 认清并理解完成任务所需要地流程；
+ 文本处理的流程。
+ 词向量嵌入的原理和实践方法。

因此主要时间花在如何处理数据上。模型结构上相比前几次实验反而要简单一些。