# 实验五：图卷积神经网络

姓名：刘威

学号：PB18010469

Click [here](#完) to finish reading:-)


## 实验目的

+ 熟悉图卷积神经网络的基本原理
+ 了解网络层数对图卷积神经网络性能的影响
+ 了解不同激活函数，Add self loop, DropEdge, PairNorm等技术对图卷积神经网络性能的影响。


## 实验原理

![image-20210621110149625](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621110149625.png)

![image-20210621110122757](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621110122757.png)

![image-20210621110050539](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621110050539-1624244452275.png)

![image-20210621110306141](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621110306141.png)

![image-20210621110337194](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621110337194.png)

![image-20210621110419877](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621110419877.png)

![image-20210621110603182](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621110603182.png)

![image-20210621110647970](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621110647970.png)

![image-20210621110619585](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621110619585-1624244780237.png)

![image-20210621110714808](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621110714808.png)

![image-20210621110733499](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621110733499.png)

![image-20210621110823719](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621110823719.png)

![image-20210621110938485](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621110938485.png)

![image-20210621110951936](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621110951936-1624244992867.png)

![image-20210621111247829](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621111247829-1624245168536.png)

![image-20210621111339615](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621111339615.png)

![image-20210621111544917](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621111544917.png)

![image-20210621111633343](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621111633343.png)

![image-20210621111948461](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621111948461.png)

![image-20210621112110697](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621112110697.png)

![image-20210621112125882](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621112125882.png)


## 实验内容

![image-20210621112457787](%E5%AE%9E%E9%AA%8C%E4%BA%94%EF%BC%9A%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20210621112457787.png)


## 实验结果

+ 本实验使用`PyTorch`进行，并主要使用了`pytorch_geometric`库。
+ 本实验再`Cora`和`Citeseer`两个数据集上进行了节点分类，并比较了自环，层数，DropEdge, PairNorm，激活函数对其分类性能的影响。


### 源码结构及说明

#### 数据处理部分

**数据集概览：**

| Dataset  | Nodes | Edges | Classes | Features |
| :------: | :---: | :---: | :-----: | :------: |
| Citeseer | 3327  | 4732  |    6    |   3703   |
|   Cora   | 2708  | 5429  |    7    |   1433   |

**数据处理方法：**

按照`pytorch_geometric`的数据输入格式，将顶点关联的`features`组织成一个二维矩阵`x: shape=(Nodes, Features)`, 将图结构，即顶点的连接关系用 `COO` 格式组织成一个二维矩阵`edge_index: shape=(2, Edges)`(邻接矩阵的稀疏表示). 将标签处理为一维向量`y: shape=(Nodes,)` 其取值范围为`range(Classes)`.

通过`mask`将顶点划分为`train, val, test`. 其中`train_mask`覆盖每个类别分别20个顶点，`val_mask`覆盖除`train_mask`外的随机500个顶点，`test_mask`覆盖除前两者外的随机1000个顶点。图结构难以拆解成三个部分，因此图是一整个输入到网络中的，也即所有的顶点都会参与计算。`train_mask`的作用是，在计算损失时，将其他顶点mask掉，只计算训练顶点的损失。同样地，通过`val_mask,test_mask`我们可以分别计算`val`和`test`顶点的分类准确率。

#### 模型部分

网络用**`n_layers`层`GCN`**堆叠而成，在每层`GCN`后都紧跟一层**可选的`PairNorm`层**；除了最后一层外，每层的`PairNorm`后还有有激活函数和`dropout`，**激活函数可以选择`relu, tanh, sigmoid`**, `dropout`可以调节drop的概率p.

其中`GCN`直接使用`pytorch_geometric`库中的`GCNConv`层，它可以**通过参数`add_self_loop`设置是否添加自环**。在输入`GCN`之前，还可以通过**设置`drop_edge`的drop比例**去掉部分`edge_index`.

完整的模型定义如下：

```python
import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, PairNorm
from torch_geometric.utils import dropout_adj

activations = {
    'relu': torch.relu,
    'sigmoid': torch.sigmoid,
    'tanh': torch.tanh,
}

class GCN(torch.nn.Module):
    def __init__(self, in_channels: int, hidden_channels: int, num_classes: int,
                 n_layers: int, act: str = 'relu', add_self_loops: bool = True,
                 pair_norm: bool = True, dropout: float = .0, drop_edge: float = .0):
        super(GCN, self).__init__()
        self.dropout = dropout
        self.drop_edge = drop_edge
        self.pair_norm = pair_norm
        self.act = activations[act] if isinstance(act, str) else act

        self.conv_list = torch.nn.ModuleList()
        for i in range(n_layers):
            in_c, out_c = hidden_channels, hidden_channels
            if i == 0:
                in_c = in_channels
            elif i == n_layers - 1:
                out_c = num_classes
            self.conv_list.append(GCNConv(in_c, out_c, add_self_loops=add_self_loops))

    def forward(self, x, edge_index):
        edge_index, _ = dropout_adj(edge_index, p=self.drop_edge)

        for i, conv in enumerate(self.conv_list):
            x = conv(x, edge_index)
            if self.pair_norm:
                x = PairNorm()(x)
            if i < len(self.conv_list) - 1:
                x = self.act(x)
                x = F.dropout(x, p=self.dropout, training=self.training)

        return x
```

### 结果及分析

#### 参数设置

本实验的可选参数及其默认值为

```python
default_cfg = {
    'data_root': './GNN/',	# 数据根目录
    'data_name': 'cora',  # citeseer or cora
    'num_train_per_class': 20,  # 训练集包含的每个类别的顶点数目
    'num_val': 500,  # 验证集顶点数目
    'num_test': 1000,  # 测试集顶点数目
    'seed': 114514,
    'device': 'cuda:0',
    'epochs': 1000,
    'patience': 5,  # 早停的等待轮数
    'lr': 5e-3,
    'weight_decay': 5e-4,
    'hidden_dim': 32,	
    'n_layers': 2,
    'activations': 'relu',
    'dropout': 0.5,
    'drop_edge': 0.,
    'add_self_loop': True,
    'pair_norm': False,
    'test_only': False
}
```

其中本实验进行调节的参数及调节的范围为

```python
cfg_grid = {
    'data_name': ['citeseer', 'cora'],
    'add_self_loop': [True, False],
    'n_layers': [1, 2, 3, 5, 10],
    'drop_edge': [0, .1, .2, .3, .5],
    'pair_norm': [True, False],
    'activations': ['relu', 'tanh', 'sigmoid']
}
```

**共有600种可能的参数组合。** **在每种参数组合下**，分别训练模型，并通过验证集`val_loss`进行早停，以`val_loss`最低时的模型权重对测试集进行测试，以其分类准确率`test_acc`作为最终评价指标。

#### 结果对比分析

所有的组合下的`test_acc`结果可以在附件[`result.csv`](./result.csv)中查看，下面仅列举出部分结果。

**两个数据集上的最好结果及对应参数**

| `data_name` | `add_self_loop` | `n_layers` | `drop_edge` | `pair_norm` | `activations` | `test_acc` |
| :---------: | :-------------: | :--------: | :---------: | :---------: | :-----------: | :--------: |
|   'cora'    |      True       |     2      |     0.      |    False    |    'relu'     |   0.797    |
| 'citeseer'  |      True       |     2      |     0.      |    False    |    'relu'     |   0.685    |

> **Note**: 下面的对比均以`citeseer`数据集为例, 即`data_name='citeseer'`

**是否添加自环的对比**

Selected Compairson:

| data_name | add_self_loop | n_layers | drop_edge | pair_norm | activations | test_acc |
| --------- | ------------- | -------- | --------- | --------- | ----------- | -------- |
| citeseer  | True          | 3        | 0.0       | False     | relu        | 0.645    |
| citeseer  | False         | 3        | 0.0       | False     | relu        | 0.628    |
| citeseer  | True          | 2        | 0.0       | False     | relu        | 0.685    |
| citeseer  | False         | 2        | 0.0       | False     | relu        | 0.667    |

分析：添加自环效果好，在某些参数下提升非常显著。

**不同层数的对比**

Selected Compairson:

| data_name | add_self_loop | n_layers | drop_edge | pair_norm | activations | test_acc |
| --------- | ------------- | -------- | --------- | --------- | ----------- | -------- |
| citeseer  | True          | 1        | 0.0       | False     | relu        | 0.68     |
| citeseer  | True          | 2        | 0.0       | False     | relu        | 0.685    |
| citeseer  | True          | 3        | 0.0       | False     | relu        | 0.645    |
| citeseer  | True          | 5        | 0.0       | False     | relu        | 0.522    |
| citeseer  | True          | 10       | 0.0       | False     | relu        | 0.176    |

分析：两层效果最好， 层数多难以优化。

**drop edge的对比**

| data_name | add_self_loop | n_layers | drop_edge | pair_norm | activations | test_acc |
| --------- | ------------- | -------- | --------- | --------- | ----------- | -------- |
| citeseer  | True          | 5        | 0.0       | False     | relu        | 0.522    |
| citeseer  | True          | 5        | 0.1       | False     | relu        | 0.351    |
| citeseer  | True          | 5        | 0.2       | False     | relu        | 0.182    |
| citeseer  | True          | 5        | 0.3       | False     | relu        | 0.201    |
| citeseer  | True          | 5        | 0.5       | False     | relu        | 0.188    |
| citeseer  | True          | 3        | 0.0       | False     | relu        | 0.645    |
| citeseer  | True          | 3        | 0.1       | False     | relu        | 0.671    |
| citeseer  | True          | 3        | 0.2       | False     | relu        | 0.655    |
| citeseer  | True          | 3        | 0.3       | False     | relu        | 0.663    |
| citeseer  | True          | 3        | 0.5       | False     | relu        | 0.609    |

分析：层数少时drop edge 有点效果，层数深时效果不好。

**是否使用PairNorm的对比**

| data_name | add_self_loop | n_layers | drop_edge | pair_norm | activations | test_acc |
| --------- | ------------- | -------- | --------- | --------- | ----------- | -------- |
| citeseer  | True          | 1        | 0.0       | False     | relu        | 0.68     |
| citeseer  | True          | 1        | 0.0       | True      | relu        | 0.443    |
| citeseer  | True          | 2        | 0.0       | False     | relu        | 0.685    |
| citeseer  | True          | 2        | 0.0       | True      | relu        | 0.526    |
| citeseer  | True          | 3        | 0.0       | False     | relu        | 0.645    |
| citeseer  | True          | 3        | 0.0       | True      | relu        | 0.568    |
| citeseer  | True          | 5        | 0.0       | False     | relu        | 0.522    |
| citeseer  | True          | 5        | 0.0       | True      | relu        | 0.545    |
| citeseer  | True          | 10       | 0.0       | False     | relu        | 0.176    |
| citeseer  | True          | 10       | 0.0       | True      | relu        | 0.3      |

分析：层数少时加`PairNorm`效果变差，层数多时`PairNorm`有效果。

**不同激活函数的对比**

| data_name | add_self_loop | n_layers | drop_edge | pari_norm | activations | test_acc |
| --------- | ------------- | -------- | --------- | --------- | ----------- | -------- |
| citeseer  | True          | 2        | 0.0       | False     | relu        | 0.685    |
| citeseer  | True          | 2        | 0.0       | False     | tanh        | 0.683    |
| citeseer  | True          | 2        | 0.0       | False     | sigmoid     | 0.207    |
| citeseer  | True          | 3        | 0.0       | False     | relu        | 0.645    |
| citeseer  | True          | 3        | 0.0       | False     | tanh        | 0.667    |
| citeseer  | True          | 3        | 0.0       | False     | sigmoid     | 0.207    |
| citeseer  | True          | 5        | 0.0       | False     | relu        | 0.522    |
| citeseer  | True          | 5        | 0.0       | False     | tanh        | 0.588    |
| citeseer  | True          | 5        | 0.0       | False     | sigmoid     | 0.207    |
| citeseer  | True          | 10       | 0.0       | False     | relu        | 0.176    |
| citeseer  | True          | 10       | 0.0       | False     | tanh        | 0.472    |
| citeseer  | True          | 10       | 0.0       | False     | sigmoid     | 0.195    |

分析：2层和3层时`relu~=tanh>>sigmoid`, 3层和5层`tanh>relu>>sigmoid`, 10层`tanh>>sigmoid~=relu`。


## 实验总结

本次实验的最大收获在于了解的图神经网络的原理，以及学会了使用`torch_geometric`库。

[原GCN论文](https://arxiv.org/pdf/1609.02907.pdf)里面`citeseer`和`cora`数据集的最好结果（%）分别为 70.3 和 81.5， 我这里略差，分别是 68.5 和 79.7。其实网络结构是一样的，也是两层，用`relu`作为激活函数。我对比了一下才发现原因：**它那个数据集划分是某种特定的划分**。虽然划分比例相同，但在它那个划分下结果就是好不少，主要差别就在于这里。~~(你们这些做学术的人都在调些什么啊，dataset split is all you need ?)~~ 

##### (完)

